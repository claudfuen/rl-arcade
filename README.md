# ğŸ® RL Arcade

**Train AI agents to play classic video games using reinforcement learning.**

Watch neural networks learn to play Pong, Breakout, Super Mario Bros, and more â€” from scratch, with no human knowledge.

<p align="center">
  <img src="https://img.shields.io/badge/python-3.8+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/pytorch-2.0+-orange.svg" alt="PyTorch">
  <img src="https://img.shields.io/badge/license-MIT-green.svg" alt="License">
</p>

---

## âœ¨ Features

- ğŸ•¹ï¸ **Multiple Games** â€” Pong, Breakout, Space Invaders, Super Mario Bros
- ğŸ“Š **Live Dashboard** â€” Watch training metrics update in real-time
- ğŸ¬ **Demo Mode** â€” See the agent play periodically during training
- ğŸ§  **Clean PPO Implementation** â€” Well-documented, educational codebase
- âš¡ **Fast Training** â€” Vectorized environments for parallel data collection
- ğŸ¯ **Interactive CLI** â€” No flags to memorize, just answer prompts

---

## ğŸš€ Quick Start

### 1. Install

```bash
git clone https://github.com/claudfuen/rl-arcade.git
cd rl-arcade
pip install -r requirements.txt
```

### 2. Run

**Interactive mode (recommended):**
```bash
python3 cli.py
```

**Or quick demo:**
```bash
python3 main.py demo --env pong
```

That's it! You'll see a Pong agent go from random flailing to actually winning.

---

## ğŸ¯ Supported Games

| Game | Difficulty | Training Time | Description |
|------|------------|---------------|-------------|
| ğŸ“ **Pong** | Easy | ~10 min | Classic paddle game. Great for beginners. |
| ğŸ§± **Breakout** | Medium | ~30 min | Break bricks with a ball. Satisfying to watch. |
| ğŸ‘¾ **Space Invaders** | Medium | ~45 min | Shoot descending aliens. |
| ğŸ„ **Super Mario Bros** | Hard | ~2 hours | Navigate World 1-1. The ultimate test. |

*Training times are rough estimates for visible improvement on an M1 Mac.*

---

## ğŸ“– Usage

### Interactive CLI

Just run and follow the prompts:

```bash
python3 cli.py
```

```
==================================================
  ğŸ® RL Game Agent Trainer
==================================================

What would you like to do?

  1. ğŸ‹ï¸  Train a new agent
  2. ğŸ¬  Watch a trained agent play
  3. âš¡  Quick demo (recommended for first time)

Enter number:
```

### Command Line

```bash
# Train with live dashboard and periodic demos
python3 main.py train --env pong --timesteps 200000 --dashboard

# Watch a trained agent
python3 main.py play --env pong --checkpoint checkpoints/best_model.pt

# Quick demo
python3 main.py demo --env breakout
```

### Key Options

| Flag | Description |
|------|-------------|
| `--env` | Game: `pong`, `breakout`, `spaceinvaders`, `mario` |
| `--timesteps` | Training duration (default: 1M) |
| `--dashboard` | Show live training graphs |
| `--demo-every` | Play demo game every N updates (default: 25) |
| `--n-envs` | Parallel environments (default: 8, more = faster) |
| `--entropy` | Exploration coefficient (default: 0.02) |

---

## ğŸ§  How It Works

This project uses **Proximal Policy Optimization (PPO)**, a state-of-the-art reinforcement learning algorithm.

### The Learning Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚   1. Agent sees game screen (84x84 grayscale pixels)   â”‚
â”‚                          â†“                              â”‚
â”‚   2. Neural network outputs action probabilities        â”‚
â”‚                          â†“                              â”‚
â”‚   3. Agent takes action, receives reward                â”‚
â”‚                          â†“                              â”‚
â”‚   4. PPO updates network to increase good actions       â”‚
â”‚                          â†“                              â”‚
â”‚   5. Repeat millions of times                          â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture

```
Game Frame (210x160 RGB)
         â†“
   Preprocessing
   â€¢ Grayscale
   â€¢ Resize to 84x84
   â€¢ Stack 4 frames (for motion)
   â€¢ Normalize to [0,1]
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CNN Backbone      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚ Conv 32x8x8   â”‚ â”‚
â”‚   â”‚ Conv 64x4x4   â”‚ â”‚
â”‚   â”‚ Conv 64x3x3   â”‚ â”‚
â”‚   â”‚ FC 512        â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â†“           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Policy â”‚  â”‚ Value  â”‚
â”‚ Head  â”‚  â”‚  Head  â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â†“          â†“
 Action    State Value
 Probs     Estimate
```

### Key Concepts

| Concept | What It Does |
|---------|--------------|
| **Policy Gradient** | Learn by increasing probability of rewarded actions |
| **Value Function** | Estimate "how good" each state is |
| **Advantage** | How much better an action was than expected |
| **Clipping** | Prevent destructively large policy updates |
| **Entropy Bonus** | Encourage exploration |
| **Frame Stacking** | Give network temporal information |

---

## ğŸ“ Project Structure

```
rl-arcade/
â”œâ”€â”€ cli.py                 # Interactive CLI (start here!)
â”œâ”€â”€ main.py               # Command-line entry point
â”œâ”€â”€ config.py             # Hyperparameters
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ networks.py       # CNN actor-critic architecture
â”‚   â””â”€â”€ ppo.py           # PPO algorithm implementation
â”‚
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ wrappers.py      # Frame preprocessing
â”‚   â””â”€â”€ make_env.py      # Environment factory
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ trainer.py       # Training loop
â”‚   â””â”€â”€ callbacks.py     # Checkpointing, logging
â”‚
â””â”€â”€ visualization/
    â”œâ”€â”€ dashboard.py     # Live matplotlib plots
    â””â”€â”€ tensorboard_utils.py
```

---

## ğŸ“ˆ Training Tips

### General

- **Start with Pong** â€” It's the fastest to train and great for verifying your setup
- **Use the dashboard** â€” Watch entropy decrease and rewards increase over time
- **More environments = faster** â€” `--n-envs 16` collects data faster (if you have RAM)

### If the agent isn't learning:

1. **Increase entropy** â€” `--entropy 0.05` encourages more exploration
2. **Train longer** â€” Some games need 500k+ steps
3. **Check the dashboard** â€” Is entropy collapsing? Are losses stable?

### Typical learning progression:

```
Steps 0-10k:      Random behavior, negative rewards
Steps 10k-50k:    Agent discovers some actions matter
Steps 50k-200k:   Basic strategy emerges (tracks ball, etc.)
Steps 200k+:      Refinement, higher scores
```

---

## ğŸ› ï¸ Development

### Running TensorBoard

```bash
tensorboard --logdir logs
# Open http://localhost:6006
```

### Code Style

```bash
pip install black isort
black .
isort .
```

---

## ğŸ—ºï¸ Roadmap

- [x] Atari games (Pong, Breakout, Space Invaders)
- [x] Super Mario Bros
- [x] Interactive CLI
- [x] Live training dashboard
- [ ] Pokemon Red/Blue (Game Boy) ğŸ”œ
- [ ] Sonic the Hedgehog
- [ ] Save/load training progress
- [ ] Hyperparameter tuning guide
- [ ] Pre-trained model zoo

---

## ğŸ“š Learn More

This codebase is designed to be educational. Key files to read:

1. **`agents/ppo.py`** â€” The PPO algorithm with detailed comments
2. **`agents/networks.py`** â€” Neural network architecture
3. **`environments/wrappers.py`** â€” Frame preprocessing explained

### Recommended Reading

- [Spinning Up in Deep RL](https://spinningup.openai.com/) â€” OpenAI's RL tutorial
- [PPO Paper](https://arxiv.org/abs/1707.06347) â€” Original PPO paper
- [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures) â€” Berkeley lectures

---

## ğŸ¤ Contributing

Contributions welcome! Some ideas:

- Add new games (Tetris, Pac-Man, etc.)
- Improve training speed
- Add more visualization options
- Write tutorials

---

## ğŸ“„ License

MIT License â€” use this however you want!

---

<p align="center">
  <b>Built for learning. Have fun! ğŸ®</b>
</p>
